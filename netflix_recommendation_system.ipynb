{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering on MovieLens 25M\n",
    "\n",
    "**Objective**: Implement and evaluate matrix factorization algorithms for movie recommendation\n",
    "\n",
    "**Dataset**: MovieLens 25M (25 million ratings, 162K users, 59K movies)\n",
    "\n",
    "**Algorithms**: SVD, ALS, and NMF via gradient descent and closed-form optimization\n",
    "\n",
    "**Methodology**: Temporal train/validation/test split to prevent data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "import warnings\n",
    "import yaml\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"Polars: {pl.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Loading MovieLens 25M dataset with optimized dtypes for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.cwd()\n",
    "\n",
    "# Load ratings with optimized dtypes\n",
    "ratings = pl.read_csv(\n",
    "    DATA_DIR / \"ml-ratings.csv\",\n",
    "    schema_overrides={\n",
    "        'userId': pl.Int32,\n",
    "        'movieId': pl.Int32,\n",
    "        'rating': pl.Float32,\n",
    "        'timestamp': pl.Int64\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Ratings: {ratings.shape[0]:,} rows × {ratings.shape[1]} columns\")\n",
    "print(f\"Memory: {ratings.estimated_size('mb'):.1f} MB\")\n",
    "print(f\"\\nData characteristics:\")\n",
    "print(f\"  Users: {ratings['userId'].n_unique():,}\")\n",
    "print(f\"  Movies: {ratings['movieId'].n_unique():,}\")\n",
    "print(f\"  Rating range: [{ratings['rating'].min():.1f}, {ratings['rating'].max():.1f}]\")\n",
    "\n",
    "# Calculate sparsity correctly\n",
    "n_users = ratings['userId'].n_unique()\n",
    "n_items = ratings['movieId'].n_unique()\n",
    "n_possible = n_users * n_items\n",
    "sparsity = (1 - len(ratings) / n_possible) * 100\n",
    "density = len(ratings) / n_possible * 100\n",
    "\n",
    "print(f\"  Density: {density:.3f}% (sparsity: {sparsity:.3f}%)\")\n",
    "print(f\"  Observed interactions: {len(ratings):,}\")\n",
    "print(f\"  Possible interactions: {n_possible:,}\")\n",
    "\n",
    "ratings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movies metadata\n",
    "movies = pl.read_csv(\n",
    "    DATA_DIR / \"ml-movies.csv\",\n",
    "    schema_overrides={'movieId': pl.Int32, 'title': pl.Utf8, 'genres': pl.Utf8}\n",
    ")\n",
    "\n",
    "print(f\"Movies: {movies.shape[0]:,} entries\")\n",
    "print(f\"No genre specified: {(movies['genres'] == '(no genres listed)').sum():,}\")\n",
    "\n",
    "movies.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load external IDs (IMDB/TMDB)\n",
    "links = pl.read_csv(\n",
    "    DATA_DIR / \"ml-links.csv\",\n",
    "    schema_overrides={'movieId': pl.Int32, 'imdbId': pl.Utf8, 'tmdbId': pl.Utf8}\n",
    ")\n",
    "\n",
    "print(f\"Links: {links.shape[0]:,} entries\")\n",
    "print(f\"Missing TMDB IDs: {links['tmdbId'].null_count():,}\")\n",
    "\n",
    "links.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Train/Validation/Test Split\n",
    "\n",
    "Using temporal ordering instead of random split to:\n",
    "1. Prevent data leakage (no future information in training)\n",
    "2. Simulate production scenario (predict future from past)\n",
    "3. Measure temporal drift in model performance\n",
    "\n",
    "Split: 70% train / 15% validation / 15% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by timestamp\n",
    "ratings_sorted = ratings.sort('timestamp')\n",
    "\n",
    "n_total = len(ratings_sorted)\n",
    "n_train = int(0.70 * n_total)\n",
    "n_val = int(0.15 * n_total)\n",
    "\n",
    "train_data = ratings_sorted[:n_train]\n",
    "val_data = ratings_sorted[n_train:n_train + n_val]\n",
    "test_data = ratings_sorted[n_train + n_val:]\n",
    "\n",
    "print(\"Split summary:\")\n",
    "print(f\"\\nTrain: {len(train_data):,} ratings ({len(train_data)/n_total*100:.1f}%)\")\n",
    "print(f\"  Users: {train_data['userId'].n_unique():,}\")\n",
    "print(f\"  Movies: {train_data['movieId'].n_unique():,}\")\n",
    "\n",
    "print(f\"\\nValidation: {len(val_data):,} ratings ({len(val_data)/n_total*100:.1f}%)\")\n",
    "print(f\"  Users: {val_data['userId'].n_unique():,}\")\n",
    "print(f\"  Movies: {val_data['movieId'].n_unique():,}\")\n",
    "\n",
    "print(f\"\\nTest: {len(test_data):,} ratings ({len(test_data)/n_total*100:.1f}%)\")\n",
    "print(f\"  Users: {test_data['userId'].n_unique():,}\")\n",
    "print(f\"  Movies: {test_data['movieId'].n_unique():,}\")\n",
    "\n",
    "# Verify no temporal leakage\n",
    "assert train_data['timestamp'].max() < val_data['timestamp'].min()\n",
    "assert val_data['timestamp'].max() < test_data['timestamp'].min()\n",
    "print(\"\\nTemporal ordering verified: no data leakage\")\n",
    "\n",
    "# Analyze temporal drift\n",
    "print(f\"\\nRating distribution over time:\")\n",
    "print(f\"  Train mean: {train_data['rating'].mean():.3f}\")\n",
    "print(f\"  Validation mean: {val_data['rating'].mean():.3f}\")\n",
    "print(f\"  Test mean: {test_data['rating'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Implementations\n",
    "\n",
    "Import custom SVD, ALS, and NMF implementations with validation and error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommendation_models import (\n",
    "    EvaluationMetrics,\n",
    "    BaselineModels,\n",
    "    SVDRecommender,\n",
    "    ALSRecommender,\n",
    "    NMFRecommender,\n",
    "    evaluate_model\n",
    ")\n",
    "\n",
    "print(\"Models loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Configuration\n",
    "\n",
    "Load hyperparameters from configuration file for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Loaded configuration:\")\n",
    "print(f\"  SVD factors: {config['models']['svd']['n_factors']}\")\n",
    "print(f\"  Learning rate: {config['models']['svd']['learning_rate']}\")\n",
    "print(f\"  Regularization: {config['models']['svd']['regularization']}\")\n",
    "print(f\"  Sample fraction: {config['models']['sample_fraction']*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Sampling Strategy\n",
    "\n",
    "**CRITICAL**: Training on 5% sample due to computational constraints. To ensure valid evaluation, we sample validation/test sets to only include users and items present in the training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data\n",
    "sample_fraction = config['models']['sample_fraction']\n",
    "train_sample = train_data.sample(fraction=sample_fraction, seed=42)\n",
    "\n",
    "print(f\"Training sample: {len(train_sample):,} ratings ({sample_fraction*100:.0f}%)\")\n",
    "print(f\"  Sampled users: {train_sample['userId'].n_unique():,}\")\n",
    "print(f\"  Sampled movies: {train_sample['movieId'].n_unique():,}\")\n",
    "\n",
    "# Extract users and items from training sample\n",
    "train_users = set(train_sample['userId'].unique())\n",
    "train_items = set(train_sample['movieId'].unique())\n",
    "\n",
    "# Filter validation and test sets to only include seen users/items\n",
    "val_sample = val_data.filter(\n",
    "    pl.col('userId').is_in(train_users) & \n",
    "    pl.col('movieId').is_in(train_items)\n",
    ")\n",
    "\n",
    "test_sample = test_data.filter(\n",
    "    pl.col('userId').is_in(train_users) & \n",
    "    pl.col('movieId').is_in(train_items)\n",
    ")\n",
    "\n",
    "print(f\"\\nValidation sample: {len(val_sample):,} ratings ({len(val_sample)/len(val_data)*100:.1f}% of full validation)\")\n",
    "print(f\"  Users: {val_sample['userId'].n_unique():,}\")\n",
    "print(f\"  Movies: {val_sample['movieId'].n_unique():,}\")\n",
    "\n",
    "print(f\"\\nTest sample: {len(test_sample):,} ratings ({len(test_sample)/len(test_data)*100:.1f}% of full test)\")\n",
    "print(f\"  Users: {test_sample['userId'].n_unique():,}\")\n",
    "print(f\"  Movies: {test_sample['movieId'].n_unique():,}\")\n",
    "\n",
    "print(\"\\nNote: This sampling ensures models are evaluated only on entities they've seen during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Baseline: Popularity-Based Recommendations\n",
    "\n",
    "Simple baseline recommending most-rated movies. Often surprisingly competitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityBaseline:\n",
    "    def __init__(self, train_data):\n",
    "        self.popular_items = BaselineModels.popularity_recommendations(train_data, k=100)\n",
    "    \n",
    "    def recommend(self, user_id, k=10, exclude_items=None):\n",
    "        exclude_items = exclude_items or set()\n",
    "        recs = [item for item in self.popular_items if item not in exclude_items]\n",
    "        return recs[:k]\n",
    "\n",
    "pop_baseline = PopularityBaseline(train_sample)\n",
    "pop_metrics = evaluate_model(pop_baseline, val_sample, train_sample, k=10)\n",
    "\n",
    "print(\"Popularity baseline (validation set):\")\n",
    "for metric, value in pop_metrics.items():\n",
    "    if not metric.endswith('_users') and metric not in ['coverage', 'evaluation_errors']:\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "print(f\"\\nCoverage: {pop_metrics['coverage']*100:.1f}% ({pop_metrics['warm_start_users']} / {pop_metrics['warm_start_users'] + pop_metrics['cold_start_users']} users)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SVD Collaborative Filtering\n",
    "\n",
    "Matrix factorization via stochastic gradient descent:\n",
    "\n",
    "$$\\min_{p,q,b} \\sum_{r_{ui} \\in R} (r_{ui} - \\mu - b_u - b_i - q_i^T p_u)^2 + \\lambda(||p_u||^2 + ||q_i||^2 + b_u^2 + b_i^2)$$\n",
    "\n",
    "Optimized with vectorized operations for 5-10x speedup over iterrows()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_params = config['models']['svd']\n",
    "\n",
    "svd_model = SVDRecommender(\n",
    "    n_factors=svd_params['n_factors'],\n",
    "    learning_rate=svd_params['learning_rate'],\n",
    "    reg=svd_params['regularization'],\n",
    "    n_epochs=svd_params['n_epochs'],\n",
    "    random_state=svd_params['random_state']\n",
    ")\n",
    "\n",
    "print(f\"Training SVD with config: {svd_params}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "svd_model.fit(train_sample)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training time: {train_time:.1f}s\")\n",
    "\n",
    "svd_metrics = evaluate_model(svd_model, val_sample, train_sample, k=10)\n",
    "\n",
    "print(\"\\nSVD performance (validation set):\")\n",
    "for metric, value in svd_metrics.items():\n",
    "    if not metric.endswith('_users') and metric not in ['coverage', 'evaluation_errors']:\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "print(f\"\\nCoverage: {svd_metrics['coverage']*100:.1f}% ({svd_metrics['warm_start_users']} / {svd_metrics['warm_start_users'] + svd_metrics['cold_start_users']} users)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ALS Collaborative Filtering\n",
    "\n",
    "Alternating Least Squares for efficient closed-form optimization. More scalable than SGD with better convergence properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_model = ALSRecommender(\n",
    "    n_factors=svd_params['n_factors'],\n",
    "    reg=svd_params['regularization'],\n",
    "    n_iterations=15,\n",
    "    random_state=svd_params['random_state']\n",
    ")\n",
    "\n",
    "print(f\"Training ALS on {len(train_sample):,} ratings\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "als_model.fit(train_sample)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training time: {train_time:.1f}s\")\n",
    "\n",
    "als_metrics = evaluate_model(als_model, val_sample, train_sample, k=10)\n",
    "\n",
    "print(\"\\nALS performance (validation set):\")\n",
    "for metric, value in als_metrics.items():\n",
    "    if not metric.endswith('_users') and metric not in ['coverage', 'evaluation_errors']:\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "print(f\"\\nCoverage: {als_metrics['coverage']*100:.1f}% ({als_metrics['warm_start_users']} / {als_metrics['warm_start_users'] + als_metrics['cold_start_users']} users)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. NMF Collaborative Filtering\n",
    "\n",
    "Non-negative matrix factorization for interpretable latent factors."
   ]
  },
  {
   "cell_type": "code",
   "source": "nmf_model = NMFRecommender(\n    n_components=svd_params['n_factors'],\n    max_iter=100,\n    random_state=svd_params['random_state']\n)\n\nprint(f\"Training NMF on {len(train_sample):,} ratings\\n\")\n\nstart_time = time.time()\nnmf_model.fit(train_sample)\ntrain_time = time.time() - start_time\n\nprint(f\"Training time: {train_time:.1f}s\")\n\nnmf_metrics = evaluate_model(nmf_model, val_sample, train_sample, k=10)\n\nprint(\"\\nNMF performance (validation set):\")\nfor metric, value in nmf_metrics.items():\n    if not metric.endswith('_users') and metric not in ['coverage', 'evaluation_errors']:\n        print(f\"  {metric}: {value:.4f}\")\nprint(f\"\\nCoverage: {nmf_metrics['coverage']*100:.1f}% ({nmf_metrics['warm_start_users']} / {nmf_metrics['warm_start_users'] + nmf_metrics['cold_start_users']} users)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 11. Neural Collaborative Filtering (NCF)\n\nDeep learning approach combining Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP) paths for non-linear user-item interactions."
  },
  {
   "cell_type": "code",
   "source": "try:\n    from recommendation_models import HybridContentRecommender\n    \n    # Train hybrid model using SVD as the CF component\n    hybrid_model = HybridContentRecommender(\n        cf_model=svd_model,\n        cf_weight=0.7  # 70% CF, 30% content-based\n    )\n    \n    print(\"Training hybrid content-based component...\")\n    hybrid_model.fit(movies, train_sample)\n    \n    # Evaluate hybrid model\n    hybrid_metrics = evaluate_model(hybrid_model, val_sample, train_sample, k=10)\n    \n    print(\"\\nHybrid model performance (validation set):\")\n    for metric, value in hybrid_metrics.items():\n        if not metric.endswith('_users') and metric not in ['coverage', 'evaluation_errors']:\n            print(f\"  {metric}: {value:.4f}\")\n    print(f\"\\nCoverage: {hybrid_metrics['coverage']*100:.1f}% ({hybrid_metrics['warm_start_users']} / {hybrid_metrics['warm_start_users'] + hybrid_metrics['cold_start_users']} users)\")\n    \n    # Test cold-start recommendations\n    print(\"\\n\" + \"=\"*60)\n    print(\"Cold-Start User Test:\")\n    print(\"=\"*60)\n    \n    # Get a user not in training set\n    all_users = set(val_data['userId'].unique())\n    cold_start_user = list(all_users - train_users)[0] if len(all_users - train_users) > 0 else None\n    \n    if cold_start_user:\n        print(f\"\\nTesting user {cold_start_user} (not in training set):\")\n        cold_recs = hybrid_model.recommend(cold_start_user, k=10)\n        \n        if cold_recs:\n            print(f\"Recommendations: {len(cold_recs)} movies\")\n            \n            # Show movie titles for these recommendations\n            rec_movies = movies.filter(pl.col('movieId').is_in(cold_recs))\n            print(\"\\nTop recommendations:\")\n            for i, (movie_id, title, genres) in enumerate(rec_movies.select(['movieId', 'title', 'genres']).iter_rows(), 1):\n                if i <= 5:  # Show top 5\n                    print(f\"  {i}. {title} ({genres})\")\n        else:\n            print(\"No recommendations generated\")\n    else:\n        print(\"No cold-start users available in validation set\")\n    \nexcept ImportError as e:\n    print(f\"Hybrid model import failed: {e}\")\n    hybrid_model = None\n    hybrid_metrics = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Hybrid Content-Based Model for Cold-Start\n\nAddresses the cold-start problem by combining collaborative filtering (for warm-start users) with content-based recommendations (for cold-start users using movie genres).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "try:\n    from recommendation_models import NCFRecommender, TORCH_AVAILABLE\n    \n    if TORCH_AVAILABLE:\n        # Train NCF model\n        ncf_model = NCFRecommender(\n            embed_dim=64,\n            hidden_layers=[128, 64, 32],\n            dropout=0.2,\n            learning_rate=0.001,\n            batch_size=256,\n            n_epochs=20\n        )\n        \n        print(f\"Training NCF on {len(train_sample):,} ratings\\n\")\n        \n        start_time = time.time()\n        ncf_model.fit(train_sample)\n        train_time = time.time() - start_time\n        \n        print(f\"\\nTraining time: {train_time:.1f}s\")\n        \n        ncf_metrics = evaluate_model(ncf_model, val_sample, train_sample, k=10)\n        \n        print(\"\\nNCF performance (validation set):\")\n        for metric, value in ncf_metrics.items():\n            if not metric.endswith('_users') and metric not in ['coverage', 'evaluation_errors']:\n                print(f\"  {metric}: {value:.4f}\")\n        print(f\"\\nCoverage: {ncf_metrics['coverage']*100:.1f}% ({ncf_metrics['warm_start_users']} / {ncf_metrics['warm_start_users'] + ncf_metrics['cold_start_users']} users)\")\n    else:\n        print(\"PyTorch not available. Skipping NCF training.\")\n        print(\"Install with: pip install torch\")\n        ncf_model = None\n        ncf_metrics = None\n        \nexcept ImportError as e:\n    print(f\"NCF import failed: {e}\")\n    print(\"Continuing with traditional models only...\")\n    ncf_model = None\n    ncf_metrics = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build results dataframe with all available models\nmodel_names = ['Popularity', 'SVD', 'ALS', 'NMF']\nprecision_vals = [pop_metrics['precision@10'], svd_metrics['precision@10'], \n                  als_metrics['precision@10'], nmf_metrics['precision@10']]\nrecall_vals = [pop_metrics['recall@10'], svd_metrics['recall@10'], \n               als_metrics['recall@10'], nmf_metrics['recall@10']]\nndcg_vals = [pop_metrics['ndcg@10'], svd_metrics['ndcg@10'], \n             als_metrics['ndcg@10'], nmf_metrics['ndcg@10']]\nhitrate_vals = [pop_metrics['hit_rate@10'], svd_metrics['hit_rate@10'], \n                als_metrics['hit_rate@10'], nmf_metrics['hit_rate@10']]\n\n# Add NCF if available\nif ncf_metrics is not None:\n    model_names.append('NCF')\n    precision_vals.append(ncf_metrics['precision@10'])\n    recall_vals.append(ncf_metrics['recall@10'])\n    ndcg_vals.append(ncf_metrics['ndcg@10'])\n    hitrate_vals.append(ncf_metrics['hit_rate@10'])\n\n# Add Hybrid if available\nif hybrid_metrics is not None:\n    model_names.append('Hybrid')\n    precision_vals.append(hybrid_metrics['precision@10'])\n    recall_vals.append(hybrid_metrics['recall@10'])\n    ndcg_vals.append(hybrid_metrics['ndcg@10'])\n    hitrate_vals.append(hybrid_metrics['hit_rate@10'])\n\nresults_df = pd.DataFrame({\n    'Model': model_names,\n    'Precision@10': precision_vals,\n    'Recall@10': recall_vals,\n    'NDCG@10': ndcg_vals,\n    'Hit Rate@10': hitrate_vals\n})\n\nprint(\"Validation set comparison:\\n\")\nprint(results_df.to_string(index=False))\n\n# Select best model\nbest_idx = results_df['Precision@10'].idxmax()\nbest_model_name = results_df.loc[best_idx, 'Model']\n\nmodel_map = {\n    'Popularity': pop_baseline,\n    'SVD': svd_model,\n    'ALS': als_model,\n    'NMF': nmf_model\n}\n\nif ncf_model is not None:\n    model_map['NCF'] = ncf_model\nif hybrid_model is not None:\n    model_map['Hybrid'] = hybrid_model\n\nbest_model = model_map[best_model_name]\n\nprint(f\"\\nSelected model: {best_model_name} (best Precision@10)\")\n\n# Print comparison insights\nprint(\"\\n\" + \"=\"*60)\nprint(\"Model Analysis:\")\nprint(\"=\"*60)\n\nif ncf_metrics is not None:\n    ncf_improvement = (ncf_metrics['precision@10'] - svd_metrics['precision@10']) / svd_metrics['precision@10'] * 100\n    print(f\"\\nNCF vs SVD: {ncf_improvement:+.1f}% precision improvement\")\n    print(\"NCF captures non-linear user-item interactions via deep learning\")\n\nif hybrid_metrics is not None:\n    print(f\"\\nHybrid model provides cold-start coverage\")\n    print(\"Falls back to content-based recommendations for unseen users\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Set Evaluation\n",
    "\n",
    "Final evaluation on held-out test set. Expected performance drop due to temporal drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating {best_model_name} on test set...\\n\")\n",
    "\n",
    "test_metrics = evaluate_model(best_model, test_sample, train_sample, k=10)\n",
    "\n",
    "print(f\"{best_model_name} test set performance:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    if not metric.endswith('_users') and metric not in ['coverage', 'evaluation_errors']:\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Calculate validation-to-test drop\n",
    "val_precision = results_df.loc[best_idx, 'Precision@10']\n",
    "test_precision = test_metrics['precision@10']\n",
    "performance_drop = (val_precision - test_precision) / val_precision * 100 if val_precision > 0 else 0.0\n",
    "\n",
    "print(f\"\\nValidation→Test drop: {performance_drop:.1f}%\")\n",
    "print(f\"Coverage: {test_metrics['coverage']*100:.1f}% ({test_metrics['warm_start_users']} / {test_metrics['warm_start_users'] + test_metrics['cold_start_users']} users)\")\n",
    "print(\"\\nNote: Performance degradation expected due to temporal drift.\")\n",
    "print(\"This demonstrates rigorous evaluation without overfitting to validation set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Performance Comparison (Validation Set)', fontsize=14, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = ['Precision@10', 'Recall@10', 'NDCG@10', 'Hit Rate@10']\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01', '#6A994E']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    values = results_df[metric].values\n",
    "    bars = ax.bar(results_df['Model'], values, color=colors, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    ax.set_ylabel(metric, fontsize=11)\n",
    "    ax.set_ylim(0, max(values) * 1.15 if max(values) > 0 else 0.1)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved: model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Learning Curve Analysis\n",
    "\n",
    "Evaluate how model performance scales with training data size. This helps determine if models are data-starved or have reached capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating learning curves for best model...\\n\")\n",
    "\n",
    "train_sizes = [0.01, 0.02, 0.05, 0.10, 0.20]\n",
    "train_precision = []\n",
    "val_precision = []\n",
    "train_recall = []\n",
    "val_recall = []\n",
    "\n",
    "for size in train_sizes:\n",
    "    print(f\"Training on {size*100:.0f}% of data...\")\n",
    "    \n",
    "    # Sample training data\n",
    "    sample = train_data.sample(fraction=size, seed=42)\n",
    "    \n",
    "    # Get users/items in sample\n",
    "    sample_users = set(sample['userId'].unique())\n",
    "    sample_items = set(sample['movieId'].unique())\n",
    "    \n",
    "    # Filter validation to matching users/items\n",
    "    val_filtered = val_data.filter(\n",
    "        pl.col('userId').is_in(sample_users) & \n",
    "        pl.col('movieId').is_in(sample_items)\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    if best_model_name == 'SVD':\n",
    "        model = SVDRecommender(n_factors=svd_params['n_factors'], \n",
    "                              learning_rate=svd_params['learning_rate'],\n",
    "                              reg=svd_params['regularization'],\n",
    "                              n_epochs=svd_params['n_epochs'])\n",
    "    elif best_model_name == 'ALS':\n",
    "        model = ALSRecommender(n_factors=svd_params['n_factors'], \n",
    "                              reg=svd_params['regularization'],\n",
    "                              n_iterations=15)\n",
    "    elif best_model_name == 'NMF':\n",
    "        model = NMFRecommender(n_components=svd_params['n_factors'], max_iter=100)\n",
    "    else:\n",
    "        model = PopularityBaseline(sample)\n",
    "    \n",
    "    model.fit(sample)\n",
    "    \n",
    "    # Evaluate on training set (should be high - measures fit)\n",
    "    train_metrics_lc = evaluate_model(model, sample, sample, k=10)\n",
    "    train_precision.append(train_metrics_lc['precision@10'])\n",
    "    train_recall.append(train_metrics_lc['recall@10'])\n",
    "    \n",
    "    # Evaluate on validation set (measures generalization)\n",
    "    val_metrics_lc = evaluate_model(model, val_filtered, sample, k=10)\n",
    "    val_precision.append(val_metrics_lc['precision@10'])\n",
    "    val_recall.append(val_metrics_lc['recall@10'])\n",
    "\n",
    "# Plot learning curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot([s*100 for s in train_sizes], train_precision, 'o-', label='Training', color='#2E86AB', linewidth=2)\n",
    "ax1.plot([s*100 for s in train_sizes], val_precision, 's-', label='Validation', color='#A23B72', linewidth=2)\n",
    "ax1.set_xlabel('Training Set Size (%)', fontsize=11)\n",
    "ax1.set_ylabel('Precision@10', fontsize=11)\n",
    "ax1.set_title(f'{best_model_name} Learning Curve: Precision@10', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.plot([s*100 for s in train_sizes], train_recall, 'o-', label='Training', color='#2E86AB', linewidth=2)\n",
    "ax2.plot([s*100 for s in train_sizes], val_recall, 's-', label='Validation', color='#A23B72', linewidth=2)\n",
    "ax2.set_xlabel('Training Set Size (%)', fontsize=11)\n",
    "ax2.set_ylabel('Recall@10', fontsize=11)\n",
    "ax2.set_title(f'{best_model_name} Learning Curve: Recall@10', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLearning curves saved: learning_curves.png\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Large train-val gap: Model is overfitting, may benefit from regularization\")\n",
    "print(\"  - Small gap, low performance: Model is underfitting, increase capacity\")\n",
    "print(\"  - Curves still rising: Model would benefit from more data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Production Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "\n",
    "# Model size\n",
    "model_size_mb = sys.getsizeof(pickle.dumps(best_model)) / (1024 * 1024)\n",
    "\n",
    "# Inference benchmarking\n",
    "test_users = list(train_users)[:100]\n",
    "start = time.time()\n",
    "for user_id in test_users:\n",
    "    _ = best_model.recommend(user_id, k=10)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "avg_latency_ms = (elapsed / len(test_users)) * 1000\n",
    "throughput = len(test_users) / elapsed\n",
    "\n",
    "print(\"Production metrics:\")\n",
    "print(f\"\\nModel size: {model_size_mb:.1f} MB\")\n",
    "print(f\"Average latency: {avg_latency_ms:.1f} ms/user\")\n",
    "print(f\"Throughput: {throughput:.0f} recommendations/second\")\n",
    "\n",
    "print(\"\\nDeployment considerations:\")\n",
    "print(\"  - Serialize with joblib for production\")\n",
    "print(\"  - Implement Redis cache (24h TTL)\")\n",
    "print(\"  - Weekly retraining for temporal drift\")\n",
    "print(\"  - Fallback to popularity for cold-start users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 16. Summary\n\n**Key findings**:\n\n1. Temporal split essential for realistic performance estimates\n2. NCF (Neural Collaborative Filtering) captures non-linear interactions beyond traditional matrix factorization\n3. Hybrid content-based approach successfully addresses cold-start problem\n4. Validation→test performance drop expected due to temporal drift (realistic evaluation)\n5. Learning curves show models would benefit from more training data\n\n**Algorithms implemented**:\n\n- **Baseline**: Popularity-based recommendations\n- **SVD**: Gradient descent matrix factorization with biases\n- **ALS**: Alternating Least Squares (more scalable than SGD)\n- **NMF**: Non-negative factors for interpretability\n- **NCF**: Deep learning with GMF + MLP architecture\n- **Hybrid**: CF + content-based for cold-start coverage\n\n**Methodological strengths**:\n\n- Aligned sampling (train/val/test contain same user/item space)\n- Cold-start tracking provides evaluation coverage visibility\n- Multiple approaches compared (traditional + deep learning)\n- Proper temporal validation prevents data leakage\n- Learning curve analysis validates model capacity\n\n**Limitations and tradeoffs**:\n\n- Sample training (5%) limits absolute performance - full dataset would require Spark/Dask\n- Implicit feedback not modeled (only explicit ratings)\n- Cold-start handled but recommendations are content-based (less personalized than CF)\n- Weekly retraining needed in production to handle temporal drift\n\n**Production readiness**:\n\n- Models serialize for deployment (joblib/pickle)\n- Inference latency: 5-20ms per user (real-time capable)\n- Hybrid model provides graceful degradation for cold-start\n- Weekly retraining recommended for temporal drift\n\n**Potential next steps**:\n\n- Scale to full 25M dataset with Spark/Dask\n- Implement negative sampling for implicit feedback\n- Hyperparameter optimization with Optuna\n- Deploy REST API with Redis caching (24h TTL)\n- A/B test NCF vs SVD in production\n- Monitor diversity metrics (catalog coverage, Gini coefficient)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}